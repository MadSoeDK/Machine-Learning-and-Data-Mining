{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7eadd07",
   "metadata": {},
   "source": [
    "If possible, update your sklearn version to 1.3.2 to reduce variance in the versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7826208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install scikit-learn==1.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd0a3fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 1.3.2.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from scipy.linalg import solve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e04e4b",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "From the 20Newsgroups dataset we fetch the documents belonging to three categories, which we use as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e6475d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'talk.politics.guns',\n",
    "              'sci.space']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da785a92",
   "metadata": {},
   "source": [
    "For example, the first document in the training data is the following one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a93d9448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: fcrary@ucsu.Colorado.EDU (Frank Crary)\n",
      "Subject: Re: Riddle me this...\n",
      "Nntp-Posting-Host: ucsu.colorado.edu\n",
      "Organization: University of Colorado, Boulder\n",
      "Distribution: usa\n",
      "Lines: 16\n",
      "\n",
      "In article <1r1lp1INN752@mojo.eng.umd.edu> chuck@eng.umd.edu (Chuck Harris - WA3UQV) writes:\n",
      ">>If so, why was CS often employed against tunnels in Vietnam?\n",
      "\n",
      ">CS \"tear-gas\" was used in Vietnam because it makes you wretch so hard that\n",
      ">your stomach comes out thru your throat.  Well, not quite that bad, but\n",
      ">you can't really do much to defend yourself while you are blowing cookies.\n",
      "\n",
      "I think the is BZ gas, not CS or CN. BZ gas exposure results in projectile\n",
      "vomiting, loss of essentially all muscle control, inability to concentrate\n",
      "or think rationally and fatal reactions in a significant fraction of\n",
      "the population. For that reason its use is limited to military\n",
      "applications.\n",
      "\n",
      "                                                          Frank Crary\n",
      "                                                          CU Boulder\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1af6f97",
   "metadata": {},
   "source": [
    "The classes are indicated categorically with indices from zero to two by the target vector. The target names tell us which index belongs to which class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "895b0228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1619"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28ceeda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, ..., 1, 2, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train.target\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ad3956b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'sci.space', 'talk.politics.guns']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebde71a",
   "metadata": {},
   "source": [
    "We represent the documents in a bag of word format. That is, we create a data matrix ``D`` such that ``D[j,i]=1`` if the j-th document contains the i-th feature (word), and ``D[j,i]=0`` otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "167204f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\", min_df=5,token_pattern=\"[^\\W\\d_]+\", binary=True)\n",
    "D = vectorizer.fit_transform(train.data)\n",
    "D_test = vectorizer.transform(test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9fdf2b",
   "metadata": {},
   "source": [
    "We get the allocation of feature indices to words by the following array, containing the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24b0e1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'aario', 'aaron', ..., 'zoology', 'zv', 'ÿ'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d57b38",
   "metadata": {},
   "source": [
    "For example, the word `naive` has the index 4044."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87ace193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4044])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(vectorizer.get_feature_names_out() == 'naive')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc11eaaa",
   "metadata": {},
   "source": [
    "### Exercise a \n",
    "Compute the class prior probabilities p(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6b22594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alt.atheism': 0.2964793082149475,\n",
       " 'sci.space': 0.3662754786905497,\n",
       " 'talk.politics.guns': 0.3372452130945028}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_documents = D.shape[0]\n",
    "\n",
    "class_prior_probabilities = {}\n",
    "\n",
    "for class_index, class_label in enumerate(train.target_names):\n",
    "    \"\"\"\n",
    "    Compute prior class probabilities.\n",
    "    class_index: Index of class labels array\n",
    "    class_label: 'alt.atheism' | 'sci.space' |'talk.politics.guns'\n",
    "    \"\"\"\n",
    "    # Number of documents in specific class\n",
    "    documents_in_class = np.sum(y_train == class_index)\n",
    "    \n",
    "    # The probability of the document in the specific class\n",
    "    class_prior_probabilities[class_label] = documents_in_class / total_documents\n",
    "\n",
    "class_prior_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d4c586",
   "metadata": {},
   "source": [
    "### Exercise b\n",
    "What are the log-probabilities of the word 'naive' given each class? Use Laplace smoothing with $α=1e−5α$. Note that the log is in ML as a default the natural logarithm to the base of e.\n",
    "Assuming that $x_{naive}$ denotes the random variable for the feature-word 'naive', compute the following probabilities: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "220df06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alt.atheism': -4.564346233136502,\n",
       " 'sci.space': -6.385184432774537,\n",
       " 'talk.politics.guns': -4.916322151258175}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = np.where(vectorizer.get_feature_names_out() == 'naive')[0][0]\n",
    "total_documents = D.shape[0]\n",
    "\n",
    "# Laplace smoothing\n",
    "α = 1e-5\n",
    "total_unique_words = D.shape[1]\n",
    "\n",
    "# Object to store result in\n",
    "word_class_log_probability = {}\n",
    "\n",
    "# For every class count the probability that naive occures in each class\n",
    "for class_index, class_label in enumerate(train.target_names):\n",
    "    \n",
    "    # Filter documents by each class label\n",
    "    class_documents = D[y_train == class_index]\n",
    "    \n",
    "    # Count occurrences of the word 'naive' in each class\n",
    "    word_occurrences = class_documents[:, word_index].sum()\n",
    "\n",
    "    # Number of documents(samples) in specific class\n",
    "    documents_in_class = np.sum(y_train == class_index)\n",
    "        \n",
    "    # Calculate probability with laplace smooothing\n",
    "    probability_word = (word_occurrences + α) / (documents_in_class + α * 2)\n",
    "    \n",
    "    # Store log probability for each class\n",
    "    word_class_log_probability[class_label] = np.log(probability_word)\n",
    "    \n",
    "word_class_log_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b1509d",
   "metadata": {},
   "source": [
    "### Exercise c\n",
    "Compute the class-conditioned log-probabilities $\\log⁡p(x_k,y)$ for each word and class combination. Apply the naive Bayes algorithm to compute the class-conditioned log probabilites for the first document x_0 in the training dataset. Use again the Laplace-smoothing with $α=1e−5$. \n",
    "\n",
    "$P(x_0, y)=P(x_0 | y)P(y)$\n",
    "\n",
    "Using equation 62 and 64 in the notebook to calculate the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "49a6e2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log p(x=x0, y=0) = -57210.94738605933\n",
      "log p(x=x0, y=1) = -51552.20958428776\n",
      "log p(x=x0, y=2) = -49191.18333249678\n"
     ]
    }
   ],
   "source": [
    "for class_index, class_label in enumerate(train.target_names):\n",
    "    \n",
    "    # Filter documents by class\n",
    "    documents_in_class = df[y_train == class_index]\n",
    "    num_of_documents_in_class = documents_in_class.shape[0]\n",
    "\n",
    "    # Total amount of features (words)\n",
    "    vocabulary_size = len(vectorizer.vocabulary_)\n",
    "\n",
    "    # Count the word_occurences in each class\n",
    "    word_occurences = documents_in_class.sum(axis=0)\n",
    "\n",
    "    # Class-Conditioned log-probabilities for each word in class\n",
    "    class_conditioned_probs = np.log((word_occurences + α) / (num_of_documents_in_class + α * vocabulary_size))\n",
    "\n",
    "    # Apply Naive Bayes Algorithm for x_0\n",
    "    log_prior = np.log(class_prior_probabilities[class_label])\n",
    "    log_probs_x0_y0 = log_prior + np.sum(class_conditioned_probs)\n",
    "\n",
    "    print(f\"log p(x=x0, y={class_index}) =\", log_probs_x0_y0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
