{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "592bf0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46.1106818780674, 3.5568355716394384)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1a)\n",
    "import numpy as np\n",
    "\n",
    "# Define the correct sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the function f(w, b)\n",
    "def f(w, b):\n",
    "    return -np.log(sigmoid(w + b)) - np.log(sigmoid(1.5 * w + b)) - np.log(sigmoid(-2 * w - b))\n",
    "\n",
    "# # Define the gradient of f(w, b)\n",
    "# def grad_f(w, b):\n",
    "#     sig_w_b = sigmoid(w + b)\n",
    "#     sig_15w_b = sigmoid(1.5 * w + b)\n",
    "#     sig_2w_b = sigmoid(-2 * w - b)\n",
    "#     grad_w = -((1 - sig_w_b) - 1.5 * (1 - sig_15w_b) + 2 * (1 - sig_2w_b))\n",
    "#     grad_b = -((1 - sig_w_b) - (1 - sig_15w_b) + (1 - sig_2w_b))\n",
    "#     return np.array([grad_w, grad_b])\n",
    "\n",
    "# Define the gradient of f(w, b)\n",
    "def grad_f(w, b):\n",
    "    grad_w = (-np.exp(b) + 1.5*np.exp(-0.5*w+b) - 0.5*np.exp(1.5*w+2*b) + 0.5*np.exp(0.5*w+b) - 2*np.exp(3*w+3*b) - np.exp(2*w+2*b) + 2.5*np.exp(-1.5*w)) / (np.log(10)*np.exp(0.5*w+b)*(np.exp(w+b)+1)*(np.exp(1.5*w+b)+1)*(np.exp(-2*w-b)+1))\n",
    "    grad_b = (-np.exp(b) + np.exp(b-0.5*w) + np.exp(0.5*w+b) - np.exp(3*w+3*b) + 2*np.exp(-1.5*w)) / (np.log(10)*np.exp(0.5*w+b)*(np.exp(w+b)+1)*(np.exp(1.5*w+b)+1)*(np.exp(-2*w-b)+1))\n",
    "    return np.array([grad_w, grad_b])\n",
    "\n",
    "# Define the constant step size function\n",
    "def eta_const(t, c=0.2):\n",
    "    return c\n",
    "\n",
    "# Define the gradient descent function\n",
    "def gradient_descent(grad_f, eta, w_b_0, max_iter=100):\n",
    "    w, b = w_b_0\n",
    "    values = []  # To store the function values\n",
    "    for t in range(max_iter):\n",
    "        eta_t = eta(t)\n",
    "        grad = grad_f(w, b)\n",
    "        w, b = w - eta_t * grad[0], b - eta_t * grad[1]\n",
    "        values.append(f(w, b))\n",
    "    return (w, b), values\n",
    "\n",
    "# Initialize parameters\n",
    "w_b_0 = (1, 1)\n",
    "max_iter = 100\n",
    "\n",
    "# Perform gradient descent\n",
    "(w_100, b_100), function_values = gradient_descent(grad_f, eta_const, w_b_0, max_iter)\n",
    "\n",
    "# Final function value after 100 iterations and the minimum value found\n",
    "final_function_value = function_values[-1]\n",
    "min_function_value = min(function_values)\n",
    "\n",
    "final_function_value, min_function_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ffebfae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0932277009188751, 1.0932277009188751)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1a)\n",
    "import numpy as np\n",
    "\n",
    "# Define the correct sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the function f(w, b)\n",
    "def f(w, b):\n",
    "    return -np.log(sigmoid(w + b)) - np.log(sigmoid(1.5 * w + b)) - np.log(sigmoid(-2 * w - b))\n",
    "\n",
    "# Define the gradient of f(w, b)\n",
    "def grad_f(w, b):\n",
    "    # Compute the sigmoid terms\n",
    "    sig_w_b = sigmoid(w + b)\n",
    "    sig_15w_b = sigmoid(1.5 * w + b)\n",
    "    sig_2w_b = sigmoid(-2 * w - b)\n",
    "\n",
    "    # Compute the partial derivatives\n",
    "    grad_w = - (1 - sig_w_b) - 1.5 * (1 - sig_15w_b) + 2 * (1 - sig_2w_b)\n",
    "    grad_b = - (1 - sig_w_b) - (1 - sig_15w_b) + (1 - sig_2w_b)\n",
    "\n",
    "    return np.array([grad_w, grad_b])\n",
    "\n",
    "# Define the constant step size function\n",
    "def eta_const(t, c=0.2):\n",
    "    return c\n",
    "\n",
    "# Define the gradient descent function\n",
    "def gradient_descent(grad_f, eta, w_b_0, max_iter=100):\n",
    "    w, b = w_b_0\n",
    "    values = []  # To store the function values\n",
    "    for t in range(max_iter):\n",
    "        eta_t = eta(t)\n",
    "        grad = grad_f(w, b)\n",
    "        w, b = w - eta_t * grad[0], b - eta_t * grad[1]\n",
    "        values.append(f(w, b))\n",
    "    return (w, b), values\n",
    "\n",
    "# Initialize parameters\n",
    "w_b_0 = (1, 1)\n",
    "max_iter = 100\n",
    "\n",
    "# Perform gradient descent\n",
    "(w_100, b_100), function_values = gradient_descent(grad_f, eta_const, w_b_0, max_iter)\n",
    "\n",
    "# Final function value after 100 iterations and the minimum value found\n",
    "final_function_value = function_values[-1]\n",
    "min_function_value = min(function_values)\n",
    "\n",
    "final_function_value, min_function_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e32e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.696568333337816, 1.696568333337816)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1b)\n",
    "# Define the decreasing step size function\n",
    "def eta_sqrt(t, c=0.2):\n",
    "    return c / np.sqrt(t + 1) \n",
    "\n",
    "# Perform gradient descent with the new step size policy\n",
    "(w_100_decreasing, b_100_decreasing), function_values_decreasing = gradient_descent(grad_f, eta_sqrt, w_b_0, max_iter)\n",
    "\n",
    "# Final function value after 100 iterations and the minimum value found with decreasing step size\n",
    "final_function_value_decreasing = function_values_decreasing[-1]\n",
    "min_function_value_decreasing = min(function_values_decreasing)\n",
    "\n",
    "final_function_value_decreasing, min_function_value_decreasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "449f8138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5983060979559247, 1.5983060979559247)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1c)\n",
    "# Define the multi-step step size function\n",
    "def eta_multistep(t, milestones=[20, 50, 80], c=0.2, eta_init=0.2):\n",
    "    # Determine the current step size based on the milestones\n",
    "    eta = eta_init\n",
    "    for milestone in milestones:\n",
    "        if t >= milestone:\n",
    "            eta *= c\n",
    "        else:\n",
    "            break\n",
    "    return eta\n",
    "\n",
    "# Perform gradient descent with the new multi-step step size policy\n",
    "(w_100_multistep, b_100_multistep), function_values_multistep = gradient_descent(grad_f, eta_multistep, w_b_0, max_iter)\n",
    "\n",
    "# Final function value after 100 iterations and the minimum value found with multi-step step size\n",
    "final_function_value_multistep = function_values_multistep[-1]\n",
    "min_function_value_multistep = min(function_values_multistep)\n",
    "\n",
    "final_function_value_multistep, min_function_value_multistep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42bd6cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.144714226062152, -1.0000000296199998, -1.5000000222100005)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2a)\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "# Define the function f(x)\n",
    "def f(x):\n",
    "    return 0.5 * x[0]**4 - x[0]*x[1] + x[1]**2 + x[1]*x[2] + x[2]**2\n",
    "\n",
    "# Function to compute arg min for each coordinate\n",
    "def argmin_x1(x2, x3):\n",
    "    # Minimize with respect to x1\n",
    "    result = minimize_scalar(lambda x1: f([x1, x2, x3]))\n",
    "    return result.x\n",
    "\n",
    "def argmin_x2(x1, x3):\n",
    "    # Minimize with respect to x2\n",
    "    result = minimize_scalar(lambda x2: f([x1, x2, x3]))\n",
    "    return result.x\n",
    "\n",
    "def argmin_x3(x1, x2):\n",
    "    # Minimize with respect to x3\n",
    "    result = minimize_scalar(lambda x3: f([x1, x2, x3]))\n",
    "    return result.x\n",
    "\n",
    "# Initial point\n",
    "x0 = [2, 3, 4]\n",
    "\n",
    "# Compute the arg min for each coordinate at x0\n",
    "arg_min_x1 = argmin_x1(x0[1], x0[2])\n",
    "arg_min_x2 = argmin_x2(x0[0], x0[2])\n",
    "arg_min_x3 = argmin_x3(x0[0], x0[1])\n",
    "\n",
    "arg_min_x1, arg_min_x2, arg_min_x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f6f0cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2.1544346582609926, -1.4227826071852905, 0.7113913141312377],\n",
       " array([-0.57735027, -0.38490018,  0.19245009]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2b)\n",
    "# Redefine argmin_xi functions to accept a full x vector and index i for updating\n",
    "def argmin_xi(x, i):\n",
    "    def objective(xi):\n",
    "        x_new = x.copy()\n",
    "        x_new[i] = xi\n",
    "        return f(x_new)\n",
    "    \n",
    "    result = minimize_scalar(objective)\n",
    "    return result.x\n",
    "\n",
    "# Implement the coordinate descent function\n",
    "def coordinate_descent(f, argmin_xi, x_0, max_iter=100):\n",
    "    x = np.array(x_0, dtype=float)\n",
    "    for t in range(max_iter):\n",
    "        for i in range(len(x)):\n",
    "            x[i] = argmin_xi(x, i)\n",
    "    return x\n",
    "\n",
    "# Initial point\n",
    "x_0 = [1, 20, 5]\n",
    "\n",
    "# Perform coordinate descent\n",
    "x_star = coordinate_descent(f, argmin_xi, x_0)\n",
    "\n",
    "# Now let's capture the first three coordinate update results for the first iteration\n",
    "first_iter_results = []\n",
    "x_temp = np.array(x_0, dtype=float)\n",
    "for i in range(3):\n",
    "    x_temp[i] = argmin_xi(x_temp, i)\n",
    "    first_iter_results.append(x_temp[i])\n",
    "\n",
    "first_iter_results, x_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74c42f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9224368884326622, 0.5621258213362399, 0.06328854538476249)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3a)\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the dataset\n",
    "california = fetch_california_housing()\n",
    "X, y = california.data, california.target\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Compute the polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=True)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "\n",
    "# Fit a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = poly.get_feature_names_out(input_features=california.feature_names)\n",
    "\n",
    "# Retrieve the regression coefficients\n",
    "beta = model.coef_\n",
    "\n",
    "# We'll use the feature names to find the indices of the desired coefficients\n",
    "medinc_index = feature_names.tolist().index('MedInc')\n",
    "avebedrms_index = feature_names.tolist().index('AveBedrms')\n",
    "houseage_avebedrms_index = feature_names.tolist().index('HouseAge AveBedrms')\n",
    "\n",
    "# Corresponding coefficients\n",
    "beta_medinc = beta[medinc_index]\n",
    "beta_avebedrms = beta[avebedrms_index]\n",
    "beta_houseage_avebedrms = beta[houseage_avebedrms_index]\n",
    "\n",
    "(beta_medinc, beta_avebedrms, beta_houseage_avebedrms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3dc634a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9224724871539206, -0.16750690727957415, 0.06334533724790394)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3b)\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X_scaled and y are already defined and have been processed as in the previous steps\n",
    "\n",
    "# Define lambda\n",
    "lambda_penalty = 0.1\n",
    "\n",
    "# Initialize Ridge Regression model\n",
    "ridge_model = Ridge(alpha=lambda_penalty)\n",
    "\n",
    "# Fit the Ridge Regression model\n",
    "ridge_model.fit(X_poly, y)\n",
    "\n",
    "# Get the coefficients\n",
    "ridge_coefs = ridge_model.coef_\n",
    "\n",
    "# Feature names might be needed to identify the position of each required coefficient\n",
    "# Assuming feature_names are defined as in the previous steps\n",
    "\n",
    "# Retrieve the desired coefficients\n",
    "beta_medinc = ridge_coefs[feature_names.tolist().index('MedInc')]\n",
    "beta_medinc_avebedrms = ridge_coefs[feature_names.tolist().index('MedInc AveBedrms')]\n",
    "beta_houseage_avebedrms = ridge_coefs[feature_names.tolist().index('HouseAge AveBedrms')]\n",
    "\n",
    "(beta_medinc, beta_medinc_avebedrms, beta_houseage_avebedrms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b38bff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.039999999999999994, 0.006666666666666665)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4)\n",
    "import numpy as np\n",
    "\n",
    "# True value at x0\n",
    "f_star_x0 = np.tan(np.pi * 0)\n",
    "\n",
    "# Model predictions at x0\n",
    "f_D1_x0 = 0 + 0.2\n",
    "f_D2_x0 = 0 + 0.3\n",
    "f_D3_x0 = 0 + 0.1\n",
    "\n",
    "# Average model prediction at x0\n",
    "f_hat_x0 = np.mean([f_D1_x0, f_D2_x0, f_D3_x0])\n",
    "\n",
    "# Bias squared at x0\n",
    "bias_squared = (f_hat_x0 - f_star_x0) ** 2\n",
    "\n",
    "# Variance at x0\n",
    "variance = np.mean([(f_D1_x0 - f_hat_x0) ** 2, (f_D2_x0 - f_hat_x0) ** 2, (f_D3_x0 - f_hat_x0) ** 2])\n",
    "\n",
    "bias_squared, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b39596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5a)\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "\n",
    "# Fetch the training data for the specified categories\n",
    "categories = ['alt.atheism', 'sci.space', 'talk.politics.guns']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# Calculate the class prior probabilities p(y)\n",
    "# This is typically the fraction of documents belonging to each class\n",
    "class_counts = np.bincount(train.target, minlength=len(categories))\n",
    "class_priors = class_counts / class_counts.sum()\n",
    "\n",
    "class_priors_dict = {train.target_names[i]: class_priors[i] for i in range(len(categories))}\n",
    "class_priors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dde27fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alt.atheism': -9.195571248744313, 'sci.space': -10.996072922516205, 'talk.politics.guns': -9.697798684037043}\n"
     ]
    }
   ],
   "source": [
    "#5b)\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'sci.space', 'talk.politics.guns']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "y_train = train.target\n",
    "\n",
    "# Use CountVectorizer to transform text data to a bag-of-words representation\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", min_df=5, token_pattern=\"[^\\W\\d_]+\", binary=True)\n",
    "D_train = vectorizer.fit_transform(train.data)\n",
    "\n",
    "# Use the feature index directly from the vectorizer\n",
    "feature_index = vectorizer.vocabulary_['naive']\n",
    "\n",
    "# Calculate the probability using Laplace smoothing for each class\n",
    "alpha = 1e-5  # Laplace smoothing parameter\n",
    "vocabulary_size = len(vectorizer.vocabulary_)  # Size of the vocabulary\n",
    "\n",
    "probabilities = {}\n",
    "for class_label in range(len(categories)):\n",
    "    class_word_counts = D_train[train.target == class_label].sum(axis=0)  # Sum counts for each word in the class\n",
    "    word_count = class_word_counts[0, feature_index]  # Count for the word \"naive\"\n",
    "    total_word_count = class_word_counts.sum()  # Total count of all words in the class\n",
    "    prob = math.log((word_count + alpha) / (total_word_count + alpha * vocabulary_size))\n",
    "    probabilities[train.target_names[class_label]] = prob\n",
    "\n",
    "# Print the probabilities\n",
    "print(probabilities)\n",
    "\n",
    "\n",
    "#antwoorden moeten nog ingevuld worden!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29ca7335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 1 ... 1 2 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-110.10545536,  -87.85047595,  -72.26902834])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5c)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "\n",
    "# Fetch the training data\n",
    "categories = ['alt.atheism', 'talk.politics.guns', 'sci.space']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Vectorize the documents\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=5, token_pattern=r\"\\b\\w+\\b\", binary=True)\n",
    "D_train = vectorizer.fit_transform(train.data)\n",
    "\n",
    "# Get the class indices\n",
    "y_train = train.target\n",
    "\n",
    "# Calculate the total number of words in the vocabulary\n",
    "total_vocab = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Laplace smoothing parameter\n",
    "alpha = 1e-5\n",
    "\n",
    "# Calculate word count per class and document count per class\n",
    "word_count_per_class = np.zeros((len(categories), total_vocab))\n",
    "doc_count_per_class = np.zeros(len(categories))\n",
    "print(y_train)\n",
    "for i in range(len(categories)):\n",
    "    # Get all documents of class i\n",
    "    class_i_docs = D_train[y_train == i]\n",
    "    # Sum across documents to get total word counts for class i\n",
    "    word_count_per_class[i, :] = class_i_docs.sum(axis=0)\n",
    "    # Count the number of documents in class i\n",
    "    doc_count_per_class[i] = class_i_docs.shape[0]\n",
    "\n",
    "# Apply Laplace smoothing to word counts and calculate log probabilities\n",
    "log_prob_x_given_y = np.log((word_count_per_class + alpha) / \n",
    "                            (doc_count_per_class[:, None] + alpha * total_vocab))\n",
    "\n",
    "# Get the word vector for the first document\n",
    "first_doc_vector = D_train[0].toarray()[0]\n",
    "\n",
    "# Calculate prior probabilities for each class\n",
    "prior_prob = np.log(doc_count_per_class / y_train.shape[0])\n",
    "\n",
    "# Calculate the log probabilities for the first document for each class\n",
    "log_probs_first_doc = prior_prob + (first_doc_vector * log_prob_x_given_y).sum(axis=1)\n",
    "\n",
    "log_probs_first_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2d61a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini impurity for the Iris dataset: 0.6667\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2]\n",
      "0.6666666666666665\n",
      "The cost of the split is -0.17476190476190478\n"
     ]
    }
   ],
   "source": [
    "#6a&b)\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "D, y = iris.data, iris.target\n",
    "\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Extract the target values (class labels)\n",
    "target = iris.target\n",
    "\n",
    "\n",
    "# Calculate the class distribution\n",
    "class_distribution = np.bincount(target)\n",
    "\n",
    "# Calculate Gini impurity\n",
    "total_samples = len(target)\n",
    "gini_impurity_old = 1 - sum((count/total_samples)**2 for count in class_distribution)\n",
    "\n",
    "print(f\"Gini impurity for the Iris dataset: {gini_impurity_old:.4f}\")\n",
    "\n",
    "# ## SVM\n",
    "\n",
    "def gini_impurity(labels):\n",
    "    \"\"\"Calculate the Gini impurity for a list of labels\"\"\"\n",
    "    unique_labels = set(labels)\n",
    "    impurity = 1.0\n",
    "    for label in unique_labels:\n",
    "        prob_of_label = np.sum(labels == label) / len(labels)\n",
    "        impurity -= prob_of_label ** 2\n",
    "    return impurity\n",
    "\n",
    "def split_cost(left_labels, right_labels):\n",
    "    \"\"\"Calculate the weighted Gini impurity for a split\"\"\"\n",
    "    n = len(left_labels) + len(right_labels)\n",
    "    print(left_labels, right_labels)\n",
    "    print(gini_impurity(left_labels+right_labels))\n",
    "    left_impurity = gini_impurity(left_labels)\n",
    "    right_impurity = gini_impurity(right_labels)\n",
    "    gini = (len(left_labels) / n) * left_impurity + (len(right_labels) / n) * right_impurity\n",
    "    return gini\n",
    "\n",
    "target_larger = []\n",
    "target_smaller = []\n",
    "mean = 5.84\n",
    "\n",
    "for i in range(len(D[:,0])):\n",
    "    sample = D[:,0][i]\n",
    "    tar = target[i]\n",
    "    if sample > mean:\n",
    "        target_larger.append(tar)\n",
    "    else: \n",
    "        target_smaller.append(tar)\n",
    "\n",
    "larger_distribution = np.bincount(target_larger)\n",
    "smaller_distribution = np.bincount(target_smaller)\n",
    "gini_new = split_cost(target_larger, target_smaller)\n",
    "# Calculate the cost of the split\n",
    "#cost = split_cost(left_labels, right_labels)\n",
    "print(f\"The cost of the split is\", gini_new-gini_impurity_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "768f75f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the SVM classifier on the test data is: 89.26%\n"
     ]
    }
   ],
   "source": [
    "#7a)\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Flatten the images\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Create target vector\n",
    "target = digits.target\n",
    "\n",
    "# Split data into 70% train and 30% test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Create a classifier: a support vector classifier with RBF kernel\n",
    "gamma_value = 0.005\n",
    "C_value = 9.0\n",
    "svm = SVC(gamma=gamma_value, C=C_value)\n",
    "\n",
    "# Learn the digits on the train subset\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict the value of the digits on the test subset\n",
    "predicted = svm.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predicted)\n",
    "print(f\"The accuracy of the SVM classifier on the test data is: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daa83d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of support vectors for classes 0 and 1: 106\n",
      "[38 68 53 62 53 56 40 62 82 78]\n"
     ]
    }
   ],
   "source": [
    "#7c)\n",
    "from sklearn import svm\n",
    "\n",
    "# Assuming `X_train` and `y_train` are your training data and labels\n",
    "# Train the SVM model\n",
    "model = svm.SVC(kernel='rbf')  # Or whichever kernel you're using\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the number of support vectors for each class\n",
    "n_support = model.n_support_\n",
    "\n",
    "# n_support_ is an array that contains the number of support vectors for each class\n",
    "# For a one-vs-one classifier, you need the support vectors for classes 0 and 1\n",
    "num_support_vectors_0_1 = n_support[0] + n_support[1]\n",
    "\n",
    "print(f\"Number of support vectors for classes 0 and 1: {num_support_vectors_0_1}\")\n",
    "print(n_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50dcea49",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'support_vectors_class_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(support_vectors_class_0[:, \u001b[38;5;241m0\u001b[39m], support_vectors_class_0[:, \u001b[38;5;241m1\u001b[39m], color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass 0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(\u001b[43msupport_vectors_class_1\u001b[49m[:, \u001b[38;5;241m0\u001b[39m], support_vectors_class_1[:, \u001b[38;5;241m1\u001b[39m], color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass 1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'support_vectors_class_1' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#7d)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(support_vectors_class_0[:, 0], support_vectors_class_0[:, 1], color='red', label='Class 0')\n",
    "plt.scatter(support_vectors_class_1[:, 0], support_vectors_class_1[:, 1], color='blue', label='Class 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb7575af",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1152 is out of bounds for axis 0 with size 592",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Select support vectors from each class\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# This would require sorting or some other selection criterion\u001b[39;00m\n\u001b[0;32m     17\u001b[0m support_vectors_class_0 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39msupport_vectors_[model\u001b[38;5;241m.\u001b[39msupport_[:\u001b[38;5;241m4\u001b[39m]]\n\u001b[1;32m---> 18\u001b[0m support_vectors_class_1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupport_vectors_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupport_\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Hypothetical labels for the plot, adjust as necessary\u001b[39;00m\n\u001b[0;32m     21\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1152 is out of bounds for axis 0 with size 592"
     ]
    }
   ],
   "source": [
    "#7d)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the support vectors for each class\n",
    "# This is a hypothetical function that would need to be adjusted to your specific case\n",
    "def plot_support_vectors(support_vectors, labels):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, sv in enumerate(support_vectors):\n",
    "        plt.subplot(2, 4, i + 1)\n",
    "        plt.imshow(sv.reshape(8, 8), cmap=plt.cm.gray)\n",
    "        plt.title(f\"Class {labels[i]} SV\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Select support vectors from each class\n",
    "# This would require sorting or some other selection criterion\n",
    "support_vectors_class_0 = model.support_vectors_[model.support_[:4]]\n",
    "support_vectors_class_1 = model.support_vectors_[model.support_[-4:]]\n",
    "\n",
    "# Hypothetical labels for the plot, adjust as necessary\n",
    "labels = [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "\n",
    "# Plotting the support vectors\n",
    "plot_support_vectors(np.vstack((support_vectors_class_0, support_vectors_class_1)), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07e687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7d)\n",
    "# After fitting the SVM model\n",
    "support_vectors = model.support_vectors_\n",
    "support_indices = model.support_\n",
    "\n",
    "# Get the support vector indices for classes 0 and 1\n",
    "support_vectors_class_0 = support_vectors[support_indices[y_train[support_indices] == 0]]\n",
    "support_vectors_class_1 = support_vectors[support_indices[y_train[support_indices] == 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ee91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7d)\n",
    "# Code to print details about the dataset and the trained SVM model.\n",
    "\n",
    "# Print the number of samples and features in the dataset\n",
    "print(f\"Number of samples: {X_train.shape[0]}\")\n",
    "print(f\"Number of features per sample: {X_train.shape[1]}\")\n",
    "\n",
    "# Check if the features are scaled (we'll just print the first 5 samples for brevity)\n",
    "print(\"First 5 samples of features:\")\n",
    "print(X_train[:5])\n",
    "\n",
    "# Print details about the trained SVM model\n",
    "print(f\"Kernel used in SVM: {model.kernel}\")\n",
    "print(f\"Number of support vectors for each class: {model.n_support_}\")\n",
    "print(f\"Indices of support vectors: {model.support_[:5]}\")  # Print first 5 for brevity\n",
    "print(f\"Dual coefficients (first 5): {model.dual_coef_[:,:5]}\")  # Print first 5 for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7d0017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7e)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset (for example, the digits dataset)\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Initialize the SVC model\n",
    "svc = SVC(kernel='rbf')\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.6, 0.8, 1, 2, 4],\n",
    "    'gamma': [0.0001, 0.0005, 0.001, 0.005]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best cross-validated score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d76b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Fetch the training data for the classes specified\n",
    "categories = ['alt.atheism', 'talk.politics.guns', 'sci.space']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "\n",
    "# Vectorize the documents\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=5, token_pattern=r\"\\b\\w+\\b\", binary=True)\n",
    "D_train = vectorizer.fit_transform(train.data)\n",
    "D_test = vectorizer.transform(train.data)  # Assuming you want to transform the same data here for demonstration\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "alpha = 1e-5  # Laplace smoothing parameter\n",
    "clf = MultinomialNB(alpha=alpha)\n",
    "clf.fit(D_train, train.target)\n",
    "\n",
    "# Retrieve the class log prior probabilities\n",
    "log_prior = clf.class_log_prior_\n",
    "\n",
    "# Retrieve the log conditional probabilities of features given a class\n",
    "log_prob = clf.feature_log_prob_\n",
    "\n",
    "# Get the word counts for the first document\n",
    "first_doc_vector = D_train[0].toarray()[0]\n",
    "\n",
    "# Calculate the log probabilities for the first document for each class\n",
    "log_prob_x0_y = log_prior + np.dot(first_doc_vector, log_prob.T)\n",
    "\n",
    "log_prob_x0_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940dd8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "\n",
    "# Fetch the training data\n",
    "categories = ['alt.atheism', 'talk.politics.guns', 'sci.space']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Vectorize the documents\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=5, token_pattern=r\"\\b\\w+\\b\", binary=True)\n",
    "D_train = vectorizer.fit_transform(train.data)\n",
    "\n",
    "# Get the class indices\n",
    "y_train = train.target\n",
    "\n",
    "# Calculate the total number of words in the vocabulary\n",
    "total_vocab = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Laplace smoothing parameter\n",
    "alpha = 1e-5\n",
    "\n",
    "# Calculate word count per class and document count per class\n",
    "word_count_per_class = np.zeros((len(categories), total_vocab))\n",
    "doc_count_per_class = np.zeros(len(categories))\n",
    "\n",
    "for i in range(len(categories)):\n",
    "    # Get all documents of class i\n",
    "    class_i_docs = D_train[y_train == i]\n",
    "    # Sum across documents to get total word counts for class i\n",
    "    word_count_per_class[i, :] = np.sum(class_i_docs, axis=0)\n",
    "    # Count the number of documents in class i\n",
    "    doc_count_per_class[i] = class_i_docs.shape[0]\n",
    "\n",
    "# Apply Laplace smoothing to word counts and calculate log probabilities\n",
    "log_prob_x_given_y = np.log((word_count_per_class + alpha) / \n",
    "                            (doc_count_per_class[:, np.newaxis] + alpha * total_vocab))\n",
    "\n",
    "# Get the word vector for the first document\n",
    "first_doc_vector = D_train[0].toarray()[0]\n",
    "\n",
    "# Calculate prior probabilities for each class\n",
    "prior_prob = np.log(doc_count_per_class / train.target.shape[0])\n",
    "\n",
    "# Calculate the log probabilities for the first document for each class\n",
    "log_probs_first_doc = prior_prob + np.sum(log_prob_x_given_y * first_doc_vector, axis=1)\n",
    "\n",
    "log_probs_first_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98352120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "\n",
    "# Fetch the training data\n",
    "categories = ['alt.atheism', 'talk.politics.guns', 'sci.space']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Vectorize the documents\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=5, token_pattern=r\"\\b\\w+\\b\", binary=True)\n",
    "D_train = vectorizer.fit_transform(train.data)\n",
    "\n",
    "# Get the class indices\n",
    "y_train = train.target\n",
    "\n",
    "# Calculate the total number of words in the vocabulary\n",
    "total_vocab = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Laplace smoothing parameter\n",
    "alpha = 1e-5\n",
    "\n",
    "# Calculate word count per class and document count per class\n",
    "word_count_per_class = np.zeros((len(categories), total_vocab))\n",
    "doc_count_per_class = np.zeros(len(categories))\n",
    "\n",
    "for i in range(len(categories)):\n",
    "    # Get all documents of class i\n",
    "    class_i_docs = D_train[y_train == i]\n",
    "    # Sum across documents to get total word counts for class i\n",
    "    word_count_per_class[i, :] = class_i_docs.sum(axis=0)\n",
    "    # Count the number of documents in class i\n",
    "    doc_count_per_class[i] = class_i_docs.shape[0]\n",
    "\n",
    "# Apply Laplace smoothing to word counts and calculate log probabilities\n",
    "log_prob_x_given_y = np.log((word_count_per_class + alpha) / \n",
    "                            (doc_count_per_class[:, None] + alpha * total_vocab))\n",
    "\n",
    "# Get the word vector for the first document\n",
    "first_doc_vector = D_train[0].toarray()[0]\n",
    "\n",
    "# Calculate prior probabilities for each class\n",
    "prior_prob = np.log(doc_count_per_class / y_train.shape[0])\n",
    "\n",
    "# Calculate the log probabilities for the first document for each class\n",
    "log_probs_first_doc = prior_prob + (first_doc_vector * log_prob_x_given_y).sum(axis=1)\n",
    "\n",
    "log_probs_first_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dbfd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7d)\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Flatten the images\n",
    "n_samples = len(digits.images)\n",
    "X = digits.images.reshape((n_samples, -1))\n",
    "y = digits.target\n",
    "\n",
    "# Split data into 70% train and 30% test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, shuffle=False) # stratify=y has been removed for the code to function\n",
    "\n",
    "# Train the SVM model\n",
    "model = svm.SVC(kernel='rbf')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Extract support vectors\n",
    "support_vectors = model.support_vectors_\n",
    "\n",
    "# Get the indices of the support vectors for each class\n",
    "support_vector_indices = model.support_\n",
    "support_vector_labels = y_train[model.support_]\n",
    "print(support_vector_indices, support_vector_labels)\n",
    "\n",
    "# Extract the dual coefficients\n",
    "dual_coef = model.dual_coef_\n",
    "\n",
    "# Find the support vectors for classes 0 and 1\n",
    "class_0_indices = support_vector_indices[support_vector_labels == 0]\n",
    "class_1_indices = support_vector_indices[support_vector_labels == 1]\n",
    "\n",
    "# Sort the dual coefficients to find the most influential support vectors\n",
    "sorted_coef_indices_class_0 = np.argsort(np.abs(dual_coef[0, support_vector_labels == 0]))[::-1]\n",
    "sorted_coef_indices_class_1 = np.argsort(np.abs(dual_coef[0, support_vector_labels == 1]))[::-1]\n",
    "\n",
    "# Select four most influential support vectors for each class\n",
    "top_sv_class_0 = class_0_indices[sorted_coef_indices_class_0[:4]]\n",
    "top_sv_class_1 = class_1_indices[sorted_coef_indices_class_1[:4]]\n",
    "\n",
    "# Plotting the most influential support vectors for each class\n",
    "fig, axes = plt.subplots(2, 4, figsize=(10, 5))\n",
    "for ax, sv_index in zip(axes[0], top_sv_class_0):\n",
    "    ax.imshow(X_train[sv_index].reshape(8, 8), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title(f\"Class 0 SV\")\n",
    "for ax, sv_index in zip(axes[1], top_sv_class_1):\n",
    "    ax.imshow(X_train[sv_index].reshape(8, 8), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title(f\"Class 1 SV\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8092c314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
